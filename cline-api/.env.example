# Server Configuration
PORT=3000
NODE_ENV=production

# API Authentication
API_KEY=your-api-key-here

# LLM Provider Configuration (choose one or multiple)
# OpenRouter (supports multiple models including free ones)
OPENROUTER_API_KEY=sk-or-v1-your-openrouter-api-key

# Anthropic (Claude)
ANTHROPIC_API_KEY=your-anthropic-api-key

# OpenAI (GPT)
OPENAI_API_KEY=your-openai-api-key

# Default LLM Provider (openrouter, anthropic, openai)
DEFAULT_LLM_PROVIDER=openrouter

# Default Model (for OpenRouter, try: x-ai/grok-beta, meta-llama/llama-3.2-3b-instruct:free, etc.)
DEFAULT_MODEL=x-ai/grok-beta

# Rate Limiting
RATE_LIMIT_WINDOW=15
RATE_LIMIT_MAX=100

# Logging
LOG_LEVEL=info

# Optional: HTTP Referer for OpenRouter (helps with rate limits)
HTTP_REFERER=http://localhost:3000

# CORS Configuration (optional)
CORS_ORIGIN=*

# Build Optimization (for Docker/Render)
YARN_CACHE_FOLDER=/opt/render/.yarn
NPM_CONFIG_PRODUCTION=true